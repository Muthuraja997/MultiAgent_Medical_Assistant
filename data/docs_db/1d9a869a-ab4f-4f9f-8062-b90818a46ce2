## CNN-based transfer learning

In transfer learning architectures, all parameters and layers outside the model are the same, but after the last 3 layers of transfer learning models are removed, layers unique to the dataset are added instead: the GlobalAveragePooling2D layer contains fewer parameters than the Flatten layer, which reduces the risk of overfitting and helps build a more efficient model. Also, while the Flatten layer is used to organize the data, the GlobalAveragePooling2D layer is used for feature extraction, making the network learning process more efficient.

Due to the fact that the training data tends to learn very fast compared to the validation data, we modified the ratio of the dropout layers in the original architectures. For all models, the dilution rate was set to 0.05. During the model training process, the designated optimizer was "RectifiedAdam", with the optimizer parameters configured as follows: learning\_rate = 0.0001, beta\_1 = 0.9, beta\_2 = 0.999, and epsilon = 1e-08. The loss function selected is categorical\_crossentropy, while the metrics used include precision, recall, categorical accuracy, and accuracy. This completes the pre-training of the model. The final layer of the model is the dense layer, which contains 4 neurons, which is usually the number of output classes in classification problems. The activation function of this layer is "softmax". The softmax function makes the output values interpretable as probabilities between classes. Furthermore, the data type of this layer is "float64", which means that the output values are of a 64-bit double precision type. The layer also applies regularization using the "kernel\_regularizer" property. The L2 regularization used here aims to reduce the risk of overfitting by limiting the size of the weights. The regularization coefficient of 0.1, denoted by "regulars. l2 (0.1)", controls the effectiveness of the regularization.

During the model training process, the "ReduceLROnPlateau" function of the Keras library was used as a backpropagation algorithm. This function automatically reduced the learning rate when the model approached a local optimum or when the loss value did not decrease. The parameters of the "ReduceLROnPlateau" function are as follows monitor: The metric monitored is usually "val\_loss" (validation loss). This is the metric used to determine if the learning rate should be reduced:

- patience: The expected patience time for lowering the learning rate, i.e. how long the metric should not improve.
- factor: The factor used to reduce the learning rate. For example, a value of 0.3 reduces the learning rate by 30%.
- min\_lr: Specifies the minimum achievable learning rate. This limits the learning rate without making it infinitesimal.

Using this feature allows for more stable and efficient model training, streamlining the process of fine-tuning training parameters without the need to manually adjust the learning rate. The training program was run over 14 epochs with batches of size 10. Details of the multilayer CNN model used in the study are presented in Fig. 2, which outlines its architectural features.

The training and validation accuracy loss graphs of the models created with VGG19, EfficientNetB4, InceptionV3 transfer learning, and CNN are shown in Fig. 3.

Table 3 shows the accuracy, F-score, Recall, Precision and AUC results of the models created in the study.

Figure 2. Multi-layer CNN model arthitecture.

picture_counter_2 The image is a diagram of a convolutional neural network (CNN) for medical image analysis. It shows an MRI brain scan as input, followed by convolutional and max pooling layers, flattening, and a fully connected layer for classification into Glioma, Meningioma, No Tumor, and Pituitary.

Vol.:(0123456789)

<!-- page_break -->

Vol:.(1234567890)

Figure 3. Learning curves of losses and accuracies of ( a ) CNN model, ( b ) EfficientNetB4 model, ( c ) VGG19 model, ( d ) InceptionV3, ( e ) VGG16 model.

picture_counter_3 The image contains five pairs of plots, each pair showing training and validation accuracy and training and validation loss versus epochs for different deep learning models: (a) CNN, (b) EfficientNetB4, (c) VGG19, (d) InceptionV3, and (e) VGG16. Each plot shows the training accuracy/loss in blue and the validation accuracy/loss in red, with the x-axis representing epochs from 1 to 14.

Table 3. Performances (%) of the models created in the study.

| Models         |   Accuracy |   F-score |   Recall |   Precision |   AUC |
|----------------|------------|-----------|----------|-------------|-------|
| VGG19          |         96 |        96 |       96 |          96 |    99 |
| EfficientNETB4 |         97 |        96 |       97 |          97 |    99 |
| InceptionV3    |         96 |        96 |       96 |          96 |    99 |
| 3 CNNModel     |         91 |        90 |       91 |          91 |    98 |
| VGG16          |         98 |        97 |       98 |          98 |    99 |

According to Table 3, the best accuracy result was obtained by VGG16 with 97%. It is ahead of other methods with F-score value of 97%, AUC value of 99%, recall value of 98% and precision values of 98%. The ROC curves of the models created in the study are shown in Fig. 4.

According to the AUC values in Fig. 4, the transfer learning models VGG, InceptionV3, and EfficientNetB4 and the models built with CNN have distinctive features. The confusion matrix of the study on the classification of glioma, meningioma, non-tumor normal patients, pituitary tumor patients in the dataset by tumor type is shown in Fig. 5.

As shown in the confusion matrix in Fig. 5, the classification performance is high for all four models (VGG16 and VGG19 models, CNN model, EfficientNetB4 model, InceptionV3 model).